---
title: "Learning From Failure"
author: "Noah Wright"
format: rtf
---

## Background

Body-worn cameras became a standard tool of policing in the mid-2010's. By recording officer interactions with the public, law enforcement agencies could achieve a greater degree of accountability. Not only could credible claims of police abuse against civilians be easily verified, the argument went, but false accusations would decline as well, saving the agency time and resources that would otherwise be wasted on spurious allegations. Initial studies seemed to support this argument, and by the end of the decade it became common practice in the United States for police to have these devices.

It was also in the late 2010's that new leadership came to the Texas Juvenile Justice Department (TJJD).[^1] Incarcerated youth are an exceptionally vulnerable population even in a well-run system. Texas did not have a well-run system. With one notable period of exception,[^2] scandal after scandal had rocked the juvenile justice system of Texas from its inception in the late 1800's through the mid 2010's. Once again, new leadership had been appointed following yet another abuse scandal. This time, however, the new Executive Director had a tool unavailable to her predecessors: body-worn cameras.

[^1]: https://www.texastribune.org/2018/01/26/texas-juvenile-justice-department-camille-cain/

[^2]: A major reform that came from a court decision, *Turman v. Morales* in 1973 kicked off the only stable period of the juvenile justice system run by the state. A single executive director, Ron Jackson, ran the agency for 20 years with a focus on reform and rehabilitation. In 1993, a bipartisan tough-on-crime effort resulted in his ousting and a rapid escalation of youth incarceration. This tragically led to a resumption of the historical norm of abuse scandals that defined the agency prior to 1973.

Secure youth residential facilities in Texas all had overhead cameras, but these were very old (they still ran on tape) and captured no audio. This presented a number of problems when it came to deciphering contested incidents, not to mention that these cameras had clearly not prevented any of the prior scandals from taking place. TJJD received special funding from the legislature to roll out body worn cameras system-wide, and after the cameras had been successfully deployed, I was tasked with seeing if staff were complying with the usage policies we had put in place.

Part of the issue was that the use of body-worn cameras was new in the field of corrections. There was not a large literature to draw from, and our first attempt at regulating and monitoring their usage fell flat on its face. This is a case study of how the Texas Juvenile Justice Department developed a methodology for comparing data on correctional officers' body-worn camera usage with data from its shift management software. This is also a case study of analysis failure, as it became clear that real-world failures of implementation were corrupting the data and rendering the methodology unusable. However, the silver lining of this failure was that it helped the agency identify previously unrecognized problems. The purpose of this case study is to demonstrate how negative findings can still be incredibly useful in real-world settings.

In 2019, body worn cameras were mostly a tool for law enforcement, and their use in a correctional environment posed a unique new challenges. TJJD had no automated mechanism for reviewing footage through machine learning, nor did the agency have the IT capacity to develop one. The only way we had to analyze footage was to view it. Unlike police officers, correctional officers deal directly with their charges for virtually their entire shift. In an 8-hour shift, a police officer might record a few calls and traffic stops. A correctional officer, on the other hand, would record 8 consecutive hours. And because TJJD recorded round-the-clock for hundreds of employees at a time, every hour of real time that passed had, on average, an eye-popping 100 hours of footage. To put that another way, reviewing a single hour of TJJD operations across all facilities would take 2.5 full work weeks.

As a result, footage review had to be done in a limited, reactive manner. If we received an incident report we could easily zero in on the cameras of the officers involved and review the incident accordingly. But our executive director had hoped to be able to use the footage proactively, looking for red flags in employees in order to prevent potential abuses instead of waiting until after those abuses had happened.

Because the agency had no way of automating the monitoring of footage contents, any proactive analysis had to be metadata based. But what to look for in the metadata? Once again, the lack of best practice literature left us in the lurch. So we brainstormed ideas for 'red flags' and came up with the following:

1.  **Minimal quantity of footage** -- our camera policy required correctional officers to have their cameras on at all times in the presence of youth.

2.  **Frequently turning the camera on and off** -- outside of bathroom breaks, a correctional officer working a dorm should not be turning their camera on and off repeatedly throughout the day.

3.  **Large gaps between clips** -- A three-hour gap in the middle of a shift should be looked into, even if it's an accident or honest mistake. It defeats the purpose of having cameras if they're not even turned on.

4.  **Mismatch between clips recorded and shifts worked** Â­-- the agency had very recently rolled out a new shift tracking software. We should expect to see the hours logged by the body cameras roughly match the hours logged as shifts worked.

## Analysis Part 1 - QC and Footage Analysis

I gathered the most recent three weeks of body worn camera data, from April 1st - April 21st of 2019 for the analysis. I also pulled the data from Shifthound (our shift management software) covering the same time period. Finally, I gathered HR data from CAPPS, the system that most of the State of Texas used at the time for personnel management and finance.[^3]

[^3]: The underlying data for the analysis as presented in this article was requested through the Texas Public Information Act and went through TJJD's approval process for ensuring anonymity of records. It is available on GitHub along with the rest of the code used to write this article.

An initial quality control review of these data sources revealed a variety of issues. See the R script below for details, but the most important ones I discovered were missing and/or bad employee ID's. Each employee had a unique identifier within CAPPS, which in turn was used to identify their assigned body camera and log their shifts. So it came as something of a surprise to find employee ID's in both the body camera and shift data that did not exist in CAPPS. Likewise, I found ID's of correctional officers in CAPPS who should have appeared in the body camera or shift data but did not. Lastly, I found ID's of staff who did appear in both systems, but were not employed at any point between April 1st - April 21st of 2019.

``` r
#Import and Format Data, Establish Parameters

library(tidyverse)
library(readxl)

Footage_Data <- read_excel("Data/Original Data/Footage Data.xlsx", col_types = "text") %>% 
  mutate(Duration_Hours = as.numeric(duration_seconds)/3600) %>% 
  rename(Footage_Job_Type = `Employee Role`)

Shift_Data <- read_excel("Data/Original Data/Shift Data.xlsx") %>% 
  mutate(`Employee ID` = as.character(`Employee ID`)) %>% 
  rename(Shift_Job_Type = `Job Type`, Shift_Location = Facility)

HR_Data <- read_csv("Data/Original Data/HR Data.csv", col_types = "c") %>% 
  mutate(across(contains("Date"), ~as.Date(.x, "%m/%d/%Y"))) %>% 
  rename(HR_Job_Type = `Job Type`, HR_Location = Location)

Secure_Facilities <- c("Alpha", "Beta", "Gamma", "Delta", "Epsilon")

#Underscore all variable names to make R coding easier

colnames(Footage_Data) <- str_replace_all(colnames(Footage_Data), " ", "_")
colnames(Shift_Data) <- str_replace_all(colnames(Shift_Data), " ", "_")
colnames(HR_Data) <- str_replace_all(colnames(HR_Data), " ", "_")

#Now that we have the data, let's start with an exploratory skim.

library(skimr)

#If you don't use skim on new data sets I don't know what you're doing.

skim(Footage_Data)
skim(HR_Data)
skim(Shift_Data)

#Looks like we have a few missing Employee ID's in footage data. Also, time
#fields in shift data have been cursed by Excel date formatting, will need to be
#exorcised later on.

#All ID's should appear in HR data. Check for ID's in footage and shift data
#that do not appear in HR data.

Footage_ID_Missing_in_HR <- Footage_Data %>% 
  select(Employee_ID) %>% 
  unique() %>% 
  na.omit() %>% 
  anti_join(HR_Data, by = "Employee_ID")

Shift_ID_Missing_in_HR <- Shift_Data %>% 
  select(Employee_ID) %>% 
  unique() %>% 
  na.omit() %>% 
  anti_join(HR_Data, by = "Employee_ID")

Invalid_IDs <- Footage_ID_Missing_in_HR %>% 
  bind_rows(Shift_ID_Missing_in_HR) %>% 
  mutate(Reason = "ID not in HR system")

#HR system is authoritative source, so these 44 ID's are errors.

#Let's see how much the data labels match up
#In other words, are the roles and locations consistent from one data set to another?

QC_Labels <- HR_Data %>% 
  left_join(Footage_Data %>% 
              select(Employee_ID, Footage_Job_Type) %>% 
              unique()) %>% 
  left_join(Shift_Data %>% 
              select(Employee_ID, Shift_Job_Type, Shift_Location) %>% 
              unique())

Label_Mismatch <- QC_Labels %>% 
  filter(Footage_Job_Type != HR_Job_Type |
           Footage_Job_Type != Shift_Job_Type |
           HR_Job_Type != Shift_Job_Type|
           HR_Location != Shift_Location) %>% 
  mutate(Reason = "Job Type or Location Label Mismatch")

#Role and location mismatch (77 ID's) minimal.
#Not a huge deal, might have just been mislabeled in shift or camera system

#So far so good, but this is the easy stuff.
#We expect to see somewhere in the footage data all Employee ID's that
#
#-are correctional (only correctional staff are required to wear cameras at all times on the dorm)
#-work at a secure facility (roll out to halfway houses was still ongoing)
#-were employed during the time frame of the study, 2019-04-01 through 2019-04-21
#-not in their first 3 months of employment, a mandatory training period prior to interacting with youth

HR_Relevant <- HR_Data %>% 
  filter(HR_Job_Type %in% c("Correctional", "Correctional Supervisor"),
         HR_Location %in% Secure_Facilities,
         is.na(Termination_Date) | Termination_Date >= "2019-04-01",
         Hire_Date <= "2019-01-21")

write_csv(HR_Relevant, "Data/Relevant HR Data.csv")

#HR data indicates 863 staff that meet this criteria. 
#How many are missing in the footage and shift data?

Missing_From_Footage_Data <- HR_Relevant %>% 
  anti_join(Footage_Data, by = "Employee_ID") %>% 
  mutate(Reason = "Missing from footage data")

Missing_From_Shift_Data <- HR_Relevant %>% 
  anti_join(Shift_Data, by = "Employee_ID") %>% 
  mutate(Reason = "Missing from shift data")

Expected_IDs_Missing <- Missing_From_Footage_Data %>% 
  bind_rows(Missing_From_Shift_Data)

#149 expected HR IDs missing in footage and 120 expected HR IDs missing from HR system. 
#269 IDs missing in total. That could definitely be better.

#What about staff whose IDs exist but were not employed during the time frame of the study?

Footage_Not_Employed <- Footage_Data %>% 
  anti_join(HR_Relevant, by = "Employee_ID") %>% 
  anti_join(Invalid_IDs) %>% 
  count(Employee_ID) %>% 
  inner_join(HR_Data) %>% 
  filter(Hire_Date > "2019-04-21"|
           Termination_Date < "2019-04-01") %>% 
  select(Employee_ID) %>% 
  unique() %>% 
  mutate(Reason = "Not employed at time of footage")

Shift_Not_Employed <- Shift_Data %>% 
  anti_join(HR_Relevant, by = "Employee_ID") %>% 
  anti_join(Invalid_IDs) %>% 
  count(Employee_ID) %>% 
  inner_join(HR_Data) %>% 
  filter(Hire_Date > "2019-04-21"|
           Termination_Date < "2019-04-01") %>% 
  select(Employee_ID) %>% 
  unique() %>% 
  mutate(Reason = "Not employed at time of shift")

Not_Employed <- Footage_Not_Employed %>% 
  bind_rows(Shift_Not_Employed)

#27 footage ID's and 13 Shift ID's appear in data but were assigned to staff not
#employed during the time frame.

#All of the problem ID's will be excluded from analysis and handed off to
#facility staff for follow-up. Label mismatches will be followed up on but not
#excluded from analysis.

Excluded_ID <- Invalid_IDs %>% 
  bind_rows(Expected_IDs_Missing) %>% 
  bind_rows(Not_Employed)

Followup_ID <- Excluded_ID %>% 
  bind_rows(Label_Mismatch)

write_csv(Followup_ID, "Output/ID's needing follow-up 1.csv")

#How much of the total quantity of footage needs to be excluded from the
#analysis for reasons of data quality?

Excluded_ID_Footage <- Footage_Data %>% 
  semi_join(Excluded_ID, by = "Employee_ID")

nrow(Excluded_ID_Footage)/nrow(Footage_Data)
sum(Excluded_ID_Footage$Duration_Hours)/sum(Footage_Data$Duration_Hours)

#11.3% of clips representing 10.6% of total footage excluded from analysis
#for data quality issues

#How much of the total quantity of footage is being excluded for any reason?
#Some footage will not be included because it does not fall under the
#"always-on" policy. For example, oversight employees (such as Monitoring or
#Inspector General staff) use cameras when interacting with youth but do not
#work full-time on the dorm. We would not expect to see their camera usage line
#up with their recorded shifts.

library(lubridate)

Analysis_Footage <- Footage_Data %>% 
  semi_join(HR_Relevant, by = "Employee_ID") %>% 
  arrange(Employee_ID, Camera_Serial_Number, created_date_record_start) %>% 
  mutate(Clip_Start = ymd_hms(created_date_record_start, tz = "US/Central"),
         Clip_End = ymd_hms(date_record_end, tz = "US/Central"))

1-nrow(Analysis_Footage)/nrow(Footage_Data)
1-sum(Analysis_Footage$Duration_Hours)/sum(Footage_Data$Duration_Hours)

#12.8% of clips representing 14.0% of total footage excluded from analysis
#for either data quality issues or employee not subject to always-on policy

#Remove irrelevant staff from Shift data and store copies of formatted tables

Shift_Format <- Shift_Data %>% 
  semi_join(HR_Relevant) %>%
  mutate(Shift_Start = ymd_hms(str_c(Start_Date, " ", str_sub(Start_Time, -8, -1)), tz = "US/Central"),
         Shift_End = ymd_hms(str_c(End_Date, " ", str_sub(End_Time, -8, -1)), tz = "US/Central"))

write_csv(Analysis_Footage, "Data/Analysis Footage.csv")
write_csv(Shift_Format, "Data/Shift Data Formatted.csv")

#Samples for reference in Quarto

write_csv(Analysis_Footage %>% 
            filter(Employee_ID == 9001005,
                   Clip_Start <= "2019-04-03") %>% 
            select(Employee_ID, Clip_ID, Clip_Start, Clip_End),
          "Output/Footage Sample.csv")

write_csv(Shift_Format %>% 
            filter(Employee_ID == 9001005,
                   Shift_Start <= "2019-04-03") %>% 
            select(Employee_ID, Shift_ID, Shift_Start, Shift_End),
          "Output/Shift Sample.csv")
```

In total, 10.6% of clips representing 11.3% of total footage had to be excluded from analysis due to these initial data quality issues.

In order to operationalize our brainstorming session, I needed to see what exactly the cameras captured in their metadata. The variables most relevant to our purposes were:

-   Clip start

-   Clip end

-   Camera Used

-   Who was assigned to the camera at the time

-   The role of the person assigned to the camera

Using these fields, I first created the following aggregations per employee ID:

**Number of clips** = Number of clips recorded.

**Days with footage** = Number of discrete dates that appear in these clips.

**Footage Hours** = Total duration of all shot footage.

**Significant Gaps** = Number of clips where the previous clip's end date was either greater than 15 minutes or less than 8 hours before this clip's start date.

I then devised the following staff metrics using the following definitions:

**Average clips per day** = Number of clips / Days with footage

**Average footage per day** = Footage hours / Days with footage

**Average clip length** = Footage hours / Number of clips

**Average gaps per day** = Gaps / Days with footage

Then I looked at the distributions of each of the 4 per-staff metrics

```{r, echo = FALSE, message = FALSE}

library(tidyverse)

Footage_Metrics_by_Employee <- read_csv("Output/Footage Metrics by Employee.csv")

Footage_Metrics_by_Employee %>% 
  select(-Clips, -Days_With_Footage, -Footage_Hours, -Gaps) %>% 
  pivot_longer(-Employee_ID, names_to = "Metric", values_to = "Value") %>% 
  ggplot(aes(x = Value)) +
  geom_histogram() +
  facet_wrap(~Metric, scales = "free")

```

By eyeballing I could tell most staff were recording fewer than 10 clips a day, shooting about 0.5-2 hours for each clip, for a total of 3-10 hours of daily footage, with the majority of employees having less than one significant gap per day. Superficially, this looked like evidence of widespread attempts at complying with the policy and no systemic rejection or resistance. If this were indeed the case, then we could turn our attention to individual outliers.

But just in case, let us see if we can validate this with another assumption. If each employee works on average 40 hours per week - a substantial underestimate given how common overtime was - we should expect to see about 120 hours of footage per employee in the dataset.

```{r, echo = FALSE, message = FALSE}

Percent_Display <- function(x) {
  str_c(100 * round(x, 3), "%")
}

Total_Footage <- sum(Footage_Metrics_by_Employee$Footage_Hours)
Total_Employees <- nrow(Footage_Metrics_by_Employee)
Average_per_Employee <- round((Total_Footage/Total_Employees),1)

print(str_c("Average Footage per Employee - ", Average_per_Employee, " hours"))

print(str_c(Average_per_Employee, " footage hours / 120 expected work hours = ", 100*Average_per_Employee/120, "% of average shift recorded"))

```

Even with the unrealistic assumption of no overtime pushing the estimated total working hours down, less than 60% of these hours appear in the footage. A perfect match would be highly unlikely, but the fact taht more than 40% of anticipated hours are unrecorded for unknown reasons warranted a follow-up.

Surely the shift data would clarify this.

## Analysis Part 2 - Footage and Shift Comparison

With the data on shifts worked from our timekeeping system, I could theoretically compare actual shifts worked to the amount of footage recorded. If there were patterns in where the footage gaps fell that might help to explain why.

In order to join the shift data to the camera data, I needed a common unit of analysis beyond "Employee ID". Using only this value would produce a nonsensical table that joined of every clip of footage to every shift worked.

For example, let's take employee #9001005 at Facility Epsilon between April 1st and April 3rd. This employee has the following clips recorded during that time period:

```{r, echo = FALSE, message = FALSE}

Sample_Clips <- read_csv("Output/Footage Sample.csv")

Sample_Clips %>% 
  knitr::kable(align = 'l')

```

We can join this to a similar table of shifts logged. This particular employee had the following shifts scheduled from April 1st, 2019 - April 3rd, 2019.

```{r, echo = FALSE, message = FALSE}

library(lubridate)

Sample_Shifts <- read_csv("Output/Shift Sample.csv")

Sample_Shifts %>% 
  knitr::kable(align = 'l')

```

Two 8-hour AM shifts from 6:00 AM to 2:00 PM. We can join the two tables together by ID on a messy many-to-many join, but that tells us nothing of how much they overlap (or fail to overlap) without extensive additional work.

We have a unique identifier for employee-clip and employee-shift, but what we need is a unique identifier that can be used to join the two. Fortunately, for this particular data we can *create* a unique identifier since both clips and shifts are fundamentally measures of *time*.

So while Employee ID is not in itself unique (i.e., one employee can have multiple clips), Employee ID combined with time of day is unique. After all, a person can only be in one place at one time!

To reshape the data for joining, I created a function that takes any data frame with a start & end column and unfolds it into discrete units of time. Using the code below creating the "Interval_Convert" function, the shift data above for employee 9001005 converts into one entry per hour of the day per shift. As a result, two 8-hour shifts get turned into 16 Employee-Hours.

``` R
{r}
library(sqldf)
library(lubridate)

Interval_Convert <- function(DF, Start_Col, End_Col, Int_Unit, Int_Length = 1) {
  
  Start_Col2 <- enquo(Start_Col)
  End_Col2 <- enquo(End_Col)
  
  Start_Col_Text <- deparse(substitute(Start_Col))
  End_Col_Text <- deparse(substitute(End_Col))
  
  Start_End <- DF %>%
    ungroup() %>%
    summarize(Min_Start = min(!!Start_Col2),
              Max_End = max(!!End_Col2)) %>%
    mutate(Start = floor_date(Min_Start, Int_Unit),
           End = ceiling_date(Max_End, Int_Unit))
  
  DF <- DF %>%
    mutate(Start_Col_Numeric = as.numeric(!!Start_Col2),
           End_Col_Numeric = as.numeric(!!End_Col2),
           Single = Start_Col_Numeric == End_Col_Numeric)
  
  Interval_Table <- data.frame(Interval_Start = seq.POSIXt(Start_End$Start[1], Start_End$End[1], by = str_c(Int_Length, " ", Int_Unit))) %>%
    mutate(Interval_End = lead(Interval_Start),
           Interval_Start_Numeric = as.numeric(Interval_Start),
           Interval_End_Numeric = as.numeric(Interval_End)) %>%
    filter(is.na(Interval_End)==FALSE)
  
  Interval_Data_Table<-sqldf('SELECT * from Interval_Table
                            LEFT JOIN DF
                            ON (Start_Col_Numeric>=Interval_Start_Numeric AND End_Col_Numeric<=Interval_End_Numeric)
                            OR (Start_Col_Numeric<Interval_Start_Numeric AND End_Col_Numeric>Interval_Start_Numeric AND End_Col_Numeric<=Interval_End_Numeric)
                            OR (Start_Col_Numeric>=Interval_Start_Numeric AND Start_Col_Numeric<Interval_End_Numeric AND End_Col_Numeric>Interval_End_Numeric)
                            OR (Start_Col_Numeric<Interval_Start_Numeric AND End_Col_Numeric>Interval_End_Numeric)') %>%
    mutate(Seconds_Duration_Within_Interval = if_else(Start_Col_Numeric>=Interval_Start_Numeric & End_Col_Numeric<=Interval_End_Numeric, End_Col_Numeric - Start_Col_Numeric,
                                      if_else(Start_Col_Numeric<=Interval_Start_Numeric & End_Col_Numeric > Interval_Start_Numeric & End_Col_Numeric <= Interval_End_Numeric, End_Col_Numeric - Interval_Start_Numeric,
                                              if_else(Start_Col_Numeric>=Interval_Start_Numeric & Start_Col_Numeric<Interval_End_Numeric & End_Col_Numeric>Interval_End_Numeric, Interval_End_Numeric - Start_Col_Numeric,
                                                      if_else(Start_Col_Numeric<Interval_Start_Numeric & End_Col_Numeric>Interval_End_Numeric, Interval_End_Numeric - Interval_Start_Numeric, Interval_End_Numeric - Interval_Start_Numeric)
                                              )))
    ) %>%
    filter(!(Single & Interval_End_Numeric == Start_Col_Numeric)) %>%
    select(-contains("Numeric"))
  
  return(Interval_Data_Table)
}
```

```{r, echo = FALSE, message = FALSE}
source("R/Interval Convert.R")

Interval_Convert(Sample_Shifts, Shift_Start, Shift_End, "hour") %>% 
  select(-Single) %>% 
  knitr::kable(align = 'l')
```

The footage can be converted in a similar manner, and this way I could break down both the shift data and the clip data into an hour-by-hour view and compare them to one another. Using this new format I joined together the full tables of footage and shifts to determine how much footage got recorded with no corresponding shift in the timekeeping system and vice-versa.

```{r, echo = FALSE, message = FALSE}

Mismatch_Statistics <- read_csv("Output/Footage-Shift Mismatch Summary Statistics.csv")

Mismatch_Statistics %>% 
  select(HR_Location, Footage_Hours_No_Shift, Employee_IDs_With_Missing_Shift) %>% 
  knitr::kable(align = 'l')
```

To summarize, almost every employee has footage hours that do not match with logged shifts, totaling nearly 14,000 hours. But what about the opposite? How many hours of shifts got logged with no corresponding footage?

```{r, echo = FALSE, message = FALSE}

Mismatch_Statistics %>% 
  select(HR_Location, Shift_Hours_No_Footage, Employee_IDs_With_Missing_Footage) %>% 
  knitr::kable(align = 'l')
```

Oh dear.

That is also almost every employee, but this time totaling about 47,000 hours. To put it another way, that's an entire work week per employee not showing up in camera footage.

At this point, we could probably rule out deliberate noncompliance. The clip data already implied that most employees were following the policy, and our facility leadership would have noticed a mass refusal large enough to show up this clearly in the data.

One way to check would be to exclude shifts that contain zero footage whatsoever. This would rule out total mismatches, where for whatever reason the logged shifts had totally failed to overlap with recorded clips.

For these footage-containing shifts, we could look at the proportion of the shift covered by the footage. So if an 8-hour shift had 4 hours of recorded footage associated with it, then we could say that 50% of the shift had been recorded. The following histogram is a distribution of the number of employees organized by the percent of their shift-hours they recorded (but only shifts that had a nonzero amount of footage).

```{r, echo = FALSE, message = FALSE}
All_Employee_Metrics <- read_csv("Output/All Employee Metrics.csv")

All_Employee_Metrics %>%
  ggplot(aes(Average_Percent_of_Other_Shifts_Recorded)) +
  geom_histogram()
```

As it turned out, most employees recorded the majority of their matching shifts, a finding that roughly matches the clip analysis.

## Causes of Failure

Here I believed we had reached the end of what I could do with data alone. I presented these findings (or lack thereof) to executive leadership, and the failure to gather reliable data from linking the clip data to the shift data prompted follow-ups into what exactly was going wrong. As it turned out, *many* things were going wrong.

First, a number of technical problems plagued the early rollout of the cameras.

-   All of our facilities suffered from high turnover, and camera ownership was not consistently updated. Employees who no longer worked at the agency could therefore appear in the clip data - somebody else had taken over their camera but not put their name and ID on it.

-   We had no way of telling if a camera was not recording due to being docked and recharging or not recording due to being switched off.

-   In the early days of the rollout, footage got assigned to an owner based on the owner of the *dock*, not the camera. In other words, if Employee A had recorded their shift but uploaded the footage using a dock assigned to Employee B then the footage would show up in the system as belonging to Employee B.

The shift data was, unsurprisingly, even worse, and it was here we came across our most important finding. While the evidence showed that there wasn't any mass non-compliance with the use of the cameras, there *was* mass non-compliance with the usage of our shift management software. To explain why we'll need to visit the reasons we even had a shift management software separate from our HR system in the first place.

Our HR system, CAPPS, had a feature that tracked hours worked in order to calculate leave and overtime pay. However, CAPPS was a statewide application designed for 9-5 office workers, and could not capture the irregular working hours of our staff (much less aid in planning future shifts).

We had obtained the shift management software to fill these gaps, but not realized how inconsistently it was being used. First, different facilities used the shift software differently. All were required to have their employees to use it, but some followed through on this better than others. And even for the ones that did make a good-faith effort at follow-through, quality control was nonexistent. In CAPPS, time entry determined pay, so strong incentives existed to ensure accurate entry. But for our shift management software, no incentives existed at all for making sure that entries were correct. For example, a correctional officer could have a 40-hour work week scheduled in the the shift software but miss the entire week due to an injury, and the software would still show them as having worked 40 hours that week. Nobody bothered to go back and correct these types of errors because there was no reason to.

## What we learned from failure

Whatever means we used to monitor compliance with the camera policy, it couldn't be fully automated. The agency followed up this analysis with a random sampling approach, in which monitors would randomly select times of day they knew a given staff member would have to have their cameras turned on and actually watch the associated clips. This review process confirmed the first impressions from the statistical review above: most employees were making good faith attempts at complying with the policy despite technical glitches, short-staffing, and administrative confusion.

As for the shift management software, we had to rethink a number of aspects of its implementation. In the process, leadership also came to clarify that its primary purpose was to help facilities schedule future shifts. As a result, ensuring accuracy after the fact was never something the agency would devote significant time and resources to, rendering it much less useful as a source for future data analysis.

This confirmed that proactive monitoring of correctional officers was a human process which had to come from supervisors and staff. We already had so many other checks in place it wasn't worth the effort to flag staff based on unreliable camera usage metrics or micromanage their after-the-fact shift logs.

The one piece of the analysis we did use going forward was the clip analysis (converted into a Power BI dashboard, included in the Github for this article), but only as a supplement for already-launched investigations, not a prompt for one. Body-worn camera footage remained immensely useful for investigations after-the-fact, but inconsistencies in clip data in and of themselves were not particularly noteworthy red flags. At the end of the day, analytics can contextualize and enhance human judgment, but it cannot replace it.

In academia, the bias in favor of positive findings is well-documented. The failure to find something or a lack of significance does not lend itself to publication in the same way that a novel discovery does. But in an applied setting, where results matter more than publication criteria, negative findings can be highly insightful. They can falsify erroneous assumptions, bring unknown problems to light, and prompt the creation of new processes and tools. In this context, a failure is only truly a failure if nothing is learned from it.
